INPUT: obs
                WERSJA A
obs = [
(float)    self.SA.current_solution_value, 
(float)    self.SA.best_solution_value,
(float)    self.SA.steps_done/self.max_steps, # (tutaj mamy ile już zrobiliśmy w %) zamienić kroki+max_kroki na % ile zostało 
(int/float)    self.getStepsWithoutCorrection(), # dodać ilość korków od ostatniej poprawy 
(float)    (self.current_temp - self.min_temp)/(self.starting_temp - self.min_temp), # dodatkowa normalizacja tempreatury. z racji na to że zakres temperatury tez jest dobierany zaleznie od zadania 
            ]


                WERSJA B (RÓŻNICA W 4. elemencie)
obs = [
            self.SA.current_solution_value, 
            self.SA.best_solution_value,
            self.SA.steps_done/self.max_steps, # (tutaj mamy ile już zrobiliśmy w %) zamienić kroki+max_kroki na % ile zostało 
            self.getStepsWithoutCorrection()/self.max_steps, # dodać ilość korków od ostatniej poprawy (dr)
            (self.current_temp - self.min_temp)/(self.starting_temp - self.min_temp), # dodatkowa normalizacja tempreatury. z racji na to że zakres temperatury tez jest dobierany zaleznie od zadania 
            ]

UWAGA po testach najleprza sięć z kategori B nie zadziała do tej pory

OUTPUT: [float(f) * 0.01 for f in range(80,121,4)]





C: (najleprzy wyuczony miał okazję uczyć się około 3 dni ciągiem.)
Dueling DQN konfiguracjai we/wy B. 

dodano minimalną nagrodę za poprawę i osłabiono karę za kroki bez poprawy

Nagroda:

# obliczamy nagrodę
        # TODO nagrody których dodanie trzeba wykonać / przemyśleć
        #! różne źródła nagród powinniśmy ze sobą nawzajem ważyć
        #? KROKI BEZ POPRAWY WARTOŚCI FUNKCJI CELU NIE POWINNY BYĆ NAGRODZONE
        # kara za zdropowanie temperatury do 0 zbyt szybko i nie podnoszenie jej przez dłuższy czas
        # kara za zakończenie ze zbyt wysoką temperaturę
        # nagroda za zakończenie z niską temperaturą
        # kara za zbyt gwałtowną zmianę temperatury (machanie góra dół lub wybieranie tylko gwałtowniejszych zmian)
        #*  nagrody które już 
        # kara za każde x kroków bez poprawy best_solution 
        # nagroda za popawę najleprzej wartości
        # kara za temperaturę przekraczającą <temp_min,starting_temp*2>

        # nagroda za poprawę best value
        reward = 2*(self.run_history[-1][1] - new_observation[1])
        if reward < 0:
            reward = -reward

        #! uwaga tutaj najbardziej newraligncze miejsce dycutujące o tym jak wygląda nagroda
        reward = math.log(reward * self.SA.steps_done + 1)*10  #reward * (math.pow(self.SA.steps_done + 1,2)/2) #(math.log(self.SA.steps_done + 1)/2)
        #* do X kroków nie oferujemy nagrody za poprawienie best value
        if self.SA.steps_done < self.no_reward_steps:
            reward = 0.0    
        
        teperature_factor = self.current_temp/self.starting_temp

        #* kara za przekroczenie granic temperaturowych
        if was_temp_lower_than_min:
            reward -= 5.0
        elif self.current_temp > self.starting_temp * 3:
            punishment = 5.0 * (int(teperature_factor)-1)
            if punishment > self.norm_reward_scale:
                punishment = self.norm_reward_scale
            reward -= punishment # silna kara za każdą krotność przekroczenia temperatur

        #* kara za każde x kroków bez poprawy best_solution 
        #! Kara została zmodyfikowana o specjalnie dobraną wartość * (1/(teperature_factor)-0.5) (sprawdź sobie wykres) nie karamy jak on stara się znaleźć nowe rozwiązanie w przypadku gdy 
        steps_without_solution_correction = self.run_history[-1][3] * self.max_steps

        if (teperature_factor <= 2):
            if (teperature_factor < 0.09):
                reward -= (steps_without_solution_correction/self.max_steps*(8))*(1/(0.09)-1) # jak trzymamy temp poniżej 0.09% / 200% możliwej to karamy max 10 razy mocniej by nie wyjebać kary zbyt dużej
            elif teperature_factor < 1:
                reward -= (steps_without_solution_correction/self.max_steps*(8))*(1/(teperature_factor)-1) # odpowieni współczyniik kary jak mamy temperaturą niższą niż startowa a utkneliśmy w minimum
            elif steps_without_solution_correction > self.max_steps*0.02:
                reward += max(15 - 30*(steps_without_solution_correction-self.max_steps*0.02)/self.max_steps,-1)  # NAGRADZAMY GO JAK PRUBUJE SZYKAĆ NOWYCH ROZWIĄZAŃ GDY DAWNO CZEGOS NIE ZNALEŹLIŚMY !!!!!!!
        else:
            reward -= (steps_without_solution_correction/self.max_steps*10) # tej sytuacji nie chcemy dodatkowo obciążać bo i tak mamy karę za zbyt dużą temperaturę 

        #! TO MOŻE NAM POMÓC Z OGARNIĘĆIEM WYBUCHAJĄCYCH WARTOŚCI PRZY STEROWANIU
        # normalizacja nagrody
        reward = max(min(reward,self.norm_reward_scale),-self.norm_reward_scale)/self.norm_reward_scale






D: (Z racji na małą zmianę między C a D, D jest uczony jako ontynuacja C z nieco poprawioną nagrodą)

konfiguracje we/wy jak w B

Nagroda:

# obliczamy nagrodę
        # TODO nagrody których dodanie trzeba wykonać / przemyśleć
        #! różne źródła nagród powinniśmy ze sobą nawzajem ważyć
        #? KROKI BEZ POPRAWY WARTOŚCI FUNKCJI CELU NIE POWINNY BYĆ NAGRODZONE
        # kara za zdropowanie temperatury do 0 zbyt szybko i nie podnoszenie jej przez dłuższy czas
        # kara za zakończenie ze zbyt wysoką temperaturę
        # nagroda za zakończenie z niską temperaturą
        # kara za zbyt gwałtowną zmianę temperatury (machanie góra dół lub wybieranie tylko gwałtowniejszych zmian)
        #*  nagrody które już 
        # kara za każde x kroków bez poprawy best_solution 
        # nagroda za popawę najleprzej wartości
        # kara za temperaturę przekraczającą <temp_min,starting_temp*2>

        # nagroda za poprawę best value
        reward = 2*(self.run_history[-1][1] - new_observation[1])
        if reward < 0:
            reward = -reward
        if reward != 0 and self.SA.steps_done > self.max_steps*0.02:
            reward += max(10.0, math.log(self.SA.steps_done) * 4.0) # dodanie pewnej minimalnej nagrody za poprawę 

        #! uwaga tutaj najbardziej newraligncze miejsce dycutujące o tym jak wygląda nagroda
        reward = math.log(reward * self.SA.steps_done + 1)*10  #reward * (math.pow(self.SA.steps_done + 1,2)/2) #(math.log(self.SA.steps_done + 1)/2)
        #* do X kroków nie oferujemy nagrody za poprawienie best value
        if self.SA.steps_done < self.no_reward_steps:
            reward = 0.0    
        
        teperature_factor = self.current_temp/self.starting_temp

        #* kara za przekroczenie granic temperaturowych
        if was_temp_lower_than_min:
            reward -= 5.0
        elif self.current_temp > self.starting_temp * 3:
            punishment = 5.0 * (int(teperature_factor)-1)
            if punishment > self.norm_reward_scale:
                punishment = self.norm_reward_scale
            reward -= punishment # silna kara za każdą krotność przekroczenia temperatur

        #* kara za każde x kroków bez poprawy best_solution 
        #! Kara została zmodyfikowana o specjalnie dobraną wartość * (1/(teperature_factor)-0.5) (sprawdź sobie wykres) nie karamy jak on stara się znaleźć nowe rozwiązanie w przypadku gdy 
        steps_without_solution_correction = self.run_history[-1][3] * self.max_steps

        if (teperature_factor <= 2):
            if (teperature_factor < 0.09):
                reward -= (steps_without_solution_correction/self.max_steps*(15))*(1/(0.09)-1) # jak trzymamy temp poniżej 0.09% / 200% możliwej to karamy max 10 razy mocniej by nie wyjebać kary zbyt dużej
            elif teperature_factor < 1:
                reward -= (steps_without_solution_correction/self.max_steps*(15))*(1/(teperature_factor)-1) # odpowieni współczyniik kary jak mamy temperaturą niższą niż startowa a utkneliśmy w minimum
            elif steps_without_solution_correction > self.max_steps*0.02:
                reward += max(15 - 30*(steps_without_solution_correction-self.max_steps*0.02)/self.max_steps,-1)  # NAGRADZAMY GO JAK PRUBUJE SZYKAĆ NOWYCH ROZWIĄZAŃ GDY DAWNO CZEGOS NIE ZNALEŹLIŚMY !!!!!!!
        else:
            reward -= (steps_without_solution_correction/self.max_steps*10) # tej sytuacji nie chcemy dodatkowo obciążać bo i tak mamy karę za zbyt dużą temperaturę 

        #! TO MOŻE NAM POMÓC Z OGARNIĘĆIEM WYBUCHAJĄCYCH WARTOŚCI PRZY STEROWANIU
        # normalizacja nagrody
        reward = max(min(reward,self.norm_reward_scale),-self.norm_reward_scale)/self.norm_reward_scale


