A: Środowisko oraz we/wy jak i nagroda są takie same jak w przypadku DuelingDQN wesja D.

Uczono na środowisku do 5000 kroków!!!!


B: nieco zmieniona architektóra PPO 

self.layer_size = 128

Do tgo nagroda znacząco zmieniona: (ogólnie uproszczona i dodano KARĘ ZA zbyd duże zmoany (szum))
        if improvement > 0:
            reward = self.norm_reward_scale * improvement
            reward = math.log1p(reward * self.SA.steps_done) * 10

            if self.SA.steps_done > self.max_steps * 0.02:
                reward = max(2.0,reward)  # bonus za poprawę po jakimś czasie #math.log(reward * self.SA.steps_done + 1)*10  #reward * (math.pow(self.SA.steps_done + 1,2)/2) #(math.log(self.SA.steps_done + 1)/2)

        #! spłaszczenie nagrody w początkowym stadium przeszukiwania
        if self.SA.steps_done < self.reward_lowerd_steps:
            reward = reward * (self.SA.steps_done/self.reward_lowerd_steps)  
        
        teperature_factor = (self.current_temp - self.min_temp) /(self.starting_temp - self.min_temp)
        self.last_temps.append(teperature_factor)

        #! dodatkowe duże kary za przekroczenie granic temperaturowych
        range_punhishment = 0
        if was_temp_lower_than_min:
            range_punhishment -= 1.0
        elif self.current_temp > self.starting_temp * 3:
            punishment = 0.5 * (int(teperature_factor)-1)
            if punishment > self.norm_reward_scale:
                punishment = self.norm_reward_scale
            range_punhishment -= punishment # silna kara za każdą krotność przekroczenia temperatur

        steps_without_solution_correction = self.run_history[-1][3] * self.max_steps

        # znaczne uproszczenie tego jak wygląda poprzednia funkcja okreslająca wa
        if steps_without_solution_correction > self.max_steps * 0.02:
            if teperature_factor < 0.2:
                range_punhishment -= 1.0 + 1.5 * (0.2 - teperature_factor)  #! lekkie wychłodzenie = mała kara
            elif teperature_factor > 1.5:
                range_punhishment -= 1.0 + (teperature_factor - 1.5) * 1.5  #! grzanie bez efektu = większa kara
            else:
                # w sensownym zakresie i próbujesz – mikro nagroda
                range_punhishment += 0.5
        reward += range_punhishment
        #! kara za stagnacje temperatury, dodatkowo kara zwiększa się aż do 5.0
        stagnation_punishment = 0
        if len(self.last_temps) >= self.last_temps.maxlen-1 and np.std(self.last_temps) < 0.012137:
            if self.stesp_of_stagnation == 0:
                self.stesp_of_stagnation = len(self.last_temps)-1
            self.stesp_of_stagnation += 1
            stagnation_punishment -= min(1.0 * self.stesp_of_stagnation / (len(self.last_temps)*2),2.5)
        else:
            self.stesp_of_stagnation = max(0,int(math.sqrt(self.stesp_of_stagnation)) -1)
            
        reward += stagnation_punishment

        delta_current_reward = 0
        #! drobna nagroda za poprawę currentalue v 
        #! Uwaga nagroda ta jest zwiększona aż do 2.0 ale za to skaluje się z odległością między current a best czyli tym bardziej go nagdzadzamy im bardziej current zbliża się do obecnego best value 
        delta_current = self.run_history[-1][0] - new_observation[0]
        if delta_current > 0:
            delta_current_reward = min(4.0* delta_current ** 0.2,2) * (1 - max(min((new_observation[0] - new_observation[1])*3.0,0.9),0.1))
        reward += delta_current_reward

        reward = max(min(reward,self.norm_reward_scale),-self.norm_reward_scale)/self.norm_reward_scale
        if self.SA.steps_done < self.max_steps:
            is_terminated = False
        else:
            is_terminated = True
            self.done = True

        self.run_history.append( new_observation + [stagnation_punishment/self.norm_reward_scale,delta_current_reward/self.norm_reward_scale,self.current_temp,reward])
        self.total_reward += reward
        return new_observation, reward , is_terminated, False, self.info()



    C   C   C   C   C   C   C   C 
    zmiana parametrów uczenia 
    Parameter	Current	Suggested
    clip_coef	0.2	    0.1
    ent_coef	0.01	0.05
    update_epch	5	    3
    min_lr	    1e-8	1e-6
    vf_coef 	0.5	    0.25
    
    ustawiamy delty na 0 na razie (tak żeby nie dodawać więcej szumu do uczenia (to chyba był błąd można sprubować teraz przeuczyć tą sieć gla poprawnych wartości w deltach ale raczej ta sieć na nie wiele się już nada)), 
    
    
    większa sieć PPO_v2
    
    
    ustawiamy dłuższe ucznie 

    PARAMETRY uczenia
            #params
        # parametry związane GAE
        self.use_gae = True
        self.gamma = 0.97
        self.gae_lambda = 0.9
        # parametry związane z lr i jego updatem
        self.starting_lr = 0.00035 
        self.min_lr = 5e-6
        self.update_lr = True
        # podstawowe okreslające uczenie
        self.seed = 1
        self.num_envs = 3
        self.num_steps = 256 # ilość symulatnicznych kroków wykonanych na środowiskach podczas jednego batcha zbieranych danych o srodowiskach
        self.num_of_minibatches = 5 #(ustaw == num_envs) dla celów nie gubienia żadnych danych i żeby się liczby ładne zgadzały
        self.total_timesteps = 50000000 # określamy łączną maksymalna ilosć korków jakie łącznie mają zostać wykonane w środowiskach
        self.lr_cycle = int(self.total_timesteps / 2)
        # batch to seria danych w uczeniu, czyli na jedną pętlę zmierzemy tyle danych łącznie, a minibatch to seria ucząća i po seri zbierania danych, rozbijamy je na num_of_minibatches podgrup aby na tej podstawie nauczyć czegoś agenta
        self.batch_size = int(self.num_envs * self.num_steps)# training_batch << batch treningu określa ile łączeni stepów środowisk ma być wykonanych na raz przed updatem sieci na podstwie tych kroków
        self.minibatch_size = int(self.batch_size // self.num_of_minibatches)# rozmiar danych uczących na jeden raz
        print("total_timesteps:",self.total_timesteps)
        self.update_epochs = 3 # uwaga tutaj ustalamy, ile razy chcemy przejść przez cały proces uczenia na tych samych danych

        self.use_adv_normalization = True # flaga która decyduje czy adventage powinno być normalizowane

        #clipping params
        self.clip_coef = 0.125 # używane do strategi clippingu zaproponowanego w PPO
        self.clip_vloss = False # ! UWAGA WYŁĄCZLIŚMY BO DEEPSEEK MÓWI ŻE LEPSZE DO RZADKO SPOTYKANYCH NAGRÓD
        self.max_grad_norm = 0.5 # maksymalna zmiana wag w sieci 

        #Entropy loss params
        self.ent_coef = 0.05 # w jakim stopniu maksymalizujemy enthropy w porównaniu do minimalizacji błędu wyjścia sieci
        self.vf_coef = 0.25 # w jakim stopniu minimalizujemy value loss w porównaniu do minimalizowania błędu na wyjściu sieci

        # parametr ograniczający zbyt duże zmiany w kolejnych iteracjach
        self.target_kl = None # defaoult_value = 0.015





        D   D   D   D D   D   D   D


        Zmiana sterowania:
        self.current_temp += self.starting_temp * (self.actions[action_number] - 1 )#= 0.9 * self.current_temp * self.actions[action_number] + self.current_temp * 0.1
        if self.current_temp < self.min_temp:
            was_temp_lower_than_min = True
            self.current_temp = self.min_temp


        NAGRODA:
        Eliminujemy karę za zbyt niską temperaturę (ta kara zbyt ogranicza agenta!!!!) wychąłdzanie to nic złego jak pokazują inne strategie


        E   E   E   E   E   E   E   E   E   E   E

        UCZENIE BEZ INFO O HISTORII CZEGOKOLWIEK NIE MA SENSU 

        OBS:
        DODAJEMY HISTORIE TEMPERATURY W POSTACI: mean, std5,std25,trend5,trend25

        Test czy zwiększenie steps per temperature do 10 zwiększy skutecznosc sieci

                                                                UWAGA TEGO NIE TESTOWANO NA POPRAWNYCH DANYCH


        F F F F F

        zmieniono nagrody na bardziej adekwatne do nowego sterowania pierwsze ustawienie parametrów nowej nagrody

        self.temp_history_size = 50
        self.temp_short_size = 10
                                                                UWAGA TEGO NIE TESTOWANO NA POPRAWNYCH DANYCH (w run test nie dodwano temp_)

        G G G G 

        zbalansowano nagrody tak aby poprawka faktycznie była znacznie większa niżwszystkie pozostałe

        self.temp_history_size = 25
        self.temp_short_size = 5








        G2 G2 G2   G2 G2 G2   G2 G2 G2   G2 G2 G2   G2 G2 G2   G2 G2 G2   G2 G2 G2   G2 G2 G2   G2 G2 G2   G2 G2 G2   G2 G2 G2   G2 G2 G2   G2 G2 G2   G2 G2 G2   G2 G2 G2   G2 G2 G2 



        dodatkowy balans nagród na podstawie tensorborda

        trendy: good_trends += 0.02 zamiast 0.1

        noImpr  - new_observation[3] * 0.025 (0.1)

        deltaC 0.0075 (0.02)


        zwiększono częstotliwość występowania kary za zbyt gwałtowne zmiany i o ciupinkę zmniejszono samą karę 

                too_fast_changes = 0
        if self.use_time_temp_info:
            if new_observation[-4]>0.01 or new_observation[-3]>0.0175: #[temp_mean, temp_std_fresh,temp_std_full, temp_trend_fresh,temp_trend_full]
                too_fast_changes -= 0.075
        reward += too_fast_changes




        #! nowe w G2
        #? drobna nagroda za utrzymywanie małych (bliskich 0) wartości trendu w okolicach wychładzania
        cold_seraching = 0
        if self.use_time_temp_info:
            if new_observation[-5]< 0.015 and abs(new_observation[-1]) < 0.005: #[temp_mean, temp_std_fresh,temp_std_full, temp_trend_fresh,temp_trend_full]
                cold_seraching += 0.005
        reward += cold_seraching
        self.total_cold_slow_changes += cold_seraching


        dodatkowo podniesiono mnożniki za kary w korkach w cieple /zimnie do 0.2 (0.175) i dodano 

        #! nowy element w G2. zerujemy karę za przebywanie w zimnie jak udało nam się faktycznie coś odnaleść
        if self.steps_without_correction <= 1:
            self.stesp_in_cold = 0
        
        if(self.stesp_in_cold > 5):#! nowy element w G2, opuźniamy karę za chłodzenie
            cold_walk_punishment -= (self.stesp_in_cold - 5)/self.SA_steps * 0.2





        


        G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3    G3


Zmiany w nagrodzie:

    HOT_COL_WALK

        naliczane gdy wystąpi +1:
            cold_walk_punishment -= 0.025 >> 0.015 
            hot_walk_punishment -= 0.025 >> 0.01

        zwiększono zakresy od których naliczają się kary:
            new_observation[-5] < 0.04 COLD
            new_observation[-5] > 0.4 HOT

        zmniejszamy wsp. naliczania dla HOT
            0.2 >> 0.15


    IMPROVMENT 

        minimana nagroda za jakąś poprawę: 1 >> 2
              ale naliczamy ją dopiero po 5% >> 20%  czasu trwania SA


    KROKI BEZ POPRAWY

        0.035 >> 0.02

    TRENDY

        nagroda zmniejszona 
        0.02 >> 0.012

    COLD SLOW CHANGES nagroda za powolne poszukiwanie w zimnym środowisku

        0.005 >> 0.012

    NOICE (kara za szum)

        Rozbicie bardzo drobne wzmocnienie 0.075 >> 0.08

        częstrze wykrywanie dla 


    delsta cuerrent (zmniejszamy bazowy wsp.)

        0.0075 >> 0.0035





    UWAGA !!!!! DUŻA ZMIANA 

        self.temp_history_size = 25 >> 40
        self.temp_short_size = 5 >> 8






Zmiany w uczeniu PPO:

        total_timesteps 100kk >>> 200kk (ogólne zwiększonie iloścki wykonanych probek)
        num_steps 256 >>> 512 (ilosc stepow wykonanych w każdym srodowisku)
        num_envs 3 >>> 10 (ilosc srodowisk odpalonych na raz)
        self.ent_coef 0.05 >>> 0.1 wzmacniamy eksplorację agenta
        update_epochs 3 >>> 1 mniej powtórzeń uczenia dla jednego zestawu danych
        num_of_minibatches 5 >>> 10

        self.batch_size = int(self.num_envs * self.num_steps)# training_batch << batch treningu określa ile łączeni stepów środowisk ma być wykonanych na raz przed updatem sieci na podstwie tych kroków
        self.minibatch_size = int(self.batch_size // self.num_of_minibatches)# rozmiar danych uczących na jeden raz





        G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4    G4


Wnioski z G3

REWARD IMPROVMENT Z ZAKRESU (60-90)



- trendy (wygląda jak utrzymana, minimalnie za mała)

    - reward_slow_clod_changes (minimalizuje nagrodę !!!!! ??????)

- reward_range (good)

    - reward_noice_short (!! mocna minimalizacji wartosci ZA DUŻE (-120 <> -40))

    - reward_noice_long (szybko udało mu się ją sprowadzić do małych wartości) (zaczą wybierać tylko słabsze sterowania, Może wypada ograniczyć ilość sterowań i zwiększyń nieco sieć ????)

- reward_no_improvment (utrzymany const szum chyba nie potrzeba bo nachodzi się z reward_improvment i może wprowadzać odatkowy szum)

        #- reward_hot (ABSURADLNIE DUŻE WARTOŚCI OKRESOWO)

        #- reward_cold (bardzo DUŻE WARTOŚCI OKRESOWO)



        CO ZMIENIONO (OSTATNIE GIGA UCZENIE)


1) uproszczenie sterowania ???

self.actions = [0.95, 0.97, 0.99, 1.0, 1.01, 1.03, 1.05]

2)



zmnieszenie kar za hot i cold walk do tego 

cold_walk_punishment -= 0.005 (trzykrotnie mniej)
hot_walk_punishment -= 0.005 (trzykrotnie mniej)

if(self.stesp_in_cold > 5):
    cold_walk_punishment -= (self.stesp_in_cold - 5)/self.SA_steps * 0.1 (dwukrotnie mniej)
hot_walk_punishment -= self.stesp_in_hot/self.SA_steps * 0.05 (trzykrotnie mniej)





0.02 >> 0.01
\/ \/ \/
reward = reward - new_observation[3] * 0.01 # ten wsp już jest znormalizowany więc kara rośnie aż do 2 (ale dowolna poprawa max value zresetuje tą karę)
        self.total_no_improvment -= new_observation[3] * 0.01 



osłabienie kar za szum 

            if new_observation[-4]>0.04:
                too_fast_changes_short -= 0.04
            if new_observation[-3]>0.03: #[temp_mean, temp_std_fresh,temp_std_full, temp_trend_fresh,temp_trend_full]
                too_fast_changes_long -= 0.04





bardzo delkiatnw wzmonienie nagrody za trendy 

0.012
\/  \/  \/
0.015



wzmonienie nagrody za małe zmiany w chłodzie 

            if new_observation[-5]< 0.0375 and abs(new_observation[-2]) < 0.0075: #[temp_mean, temp_std_fresh,temp_std_full, temp_trend_fresh,temp_trend_full]
                cold_seraching += 0.025












                G5                G5                G5                G5                G5                G5                G5                G5                G5                G5                G5                G5                G5                G5                G5                G5                G5                 H                 H                 H                 H                 H                 H                 H                 H

Parametry PPO zmienione 

uczenie znacznie dłuższe mniejsze kroki uczenia (lr mniejszy).

W envie BARDZO małe zmiany śladowe poprawki w zasadzieG6





                G6                G6                G6                G6                G6                G6                G6

        OSŁABIENIE NAGRODY ZA COLDSTEPSY, AGNET GRINDOWAŁ NAGRODĘ ZA stepsy w cold_seraching
    cold_seraching += 0.08  >>>>  cold_seraching += 0.03
                if new_observation[-5]< 0.08 and abs(new_observation[-2]) < 0.0075 and good_trends > 0.001: #[temp_mean, temp_std_fresh,temp_std_full, temp_trend_fresh,temp_trend_full]
                cold_seraching += 0.035


    dodatkowo zmiana w wyłapywaniu cold_walk_punishment
                if new_observation[-5] < 0.081: <<<< 0.04


    NAJNOWSZE OBSERWACJE SUGEROWAŁY PRZYTŁOCZENIE NAGRODĄ ZA TRENDY 

    zmniejszenie wsp. za trendy z 0.0125 >> 0.0035

            self.norm_reward_scale = 10.0 (było 8.0) trochę podnosimy nagrodę za zmalezienie lepszego rozw. 