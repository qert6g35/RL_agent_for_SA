A: Środowisko oraz we/wy jak i nagroda są takie same jak w przypadku DuelingDQN wesja D.

Uczono na środowisku do 5000 kroków!!!!


B: nieco zmieniona architektóra PPO 

self.layer_size = 128

Do tgo nagroda znacząco zmieniona: (ogólnie uproszczona i dodano KARĘ ZA zbyd duże zmoany (szum))
        if improvement > 0:
            reward = self.norm_reward_scale * improvement
            reward = math.log1p(reward * self.SA.steps_done) * 10

            if self.SA.steps_done > self.max_steps * 0.02:
                reward = max(2.0,reward)  # bonus za poprawę po jakimś czasie #math.log(reward * self.SA.steps_done + 1)*10  #reward * (math.pow(self.SA.steps_done + 1,2)/2) #(math.log(self.SA.steps_done + 1)/2)

        #! spłaszczenie nagrody w początkowym stadium przeszukiwania
        if self.SA.steps_done < self.reward_lowerd_steps:
            reward = reward * (self.SA.steps_done/self.reward_lowerd_steps)  
        
        teperature_factor = (self.current_temp - self.min_temp) /(self.starting_temp - self.min_temp)
        self.last_temps.append(teperature_factor)

        #! dodatkowe duże kary za przekroczenie granic temperaturowych
        range_punhishment = 0
        if was_temp_lower_than_min:
            range_punhishment -= 1.0
        elif self.current_temp > self.starting_temp * 3:
            punishment = 0.5 * (int(teperature_factor)-1)
            if punishment > self.norm_reward_scale:
                punishment = self.norm_reward_scale
            range_punhishment -= punishment # silna kara za każdą krotność przekroczenia temperatur

        steps_without_solution_correction = self.run_history[-1][3] * self.max_steps

        # znaczne uproszczenie tego jak wygląda poprzednia funkcja okreslająca wa
        if steps_without_solution_correction > self.max_steps * 0.02:
            if teperature_factor < 0.2:
                range_punhishment -= 1.0 + 1.5 * (0.2 - teperature_factor)  #! lekkie wychłodzenie = mała kara
            elif teperature_factor > 1.5:
                range_punhishment -= 1.0 + (teperature_factor - 1.5) * 1.5  #! grzanie bez efektu = większa kara
            else:
                # w sensownym zakresie i próbujesz – mikro nagroda
                range_punhishment += 0.5
        reward += range_punhishment
        #! kara za stagnacje temperatury, dodatkowo kara zwiększa się aż do 5.0
        stagnation_punishment = 0
        if len(self.last_temps) >= self.last_temps.maxlen-1 and np.std(self.last_temps) < 0.012137:
            if self.stesp_of_stagnation == 0:
                self.stesp_of_stagnation = len(self.last_temps)-1
            self.stesp_of_stagnation += 1
            stagnation_punishment -= min(1.0 * self.stesp_of_stagnation / (len(self.last_temps)*2),2.5)
        else:
            self.stesp_of_stagnation = max(0,int(math.sqrt(self.stesp_of_stagnation)) -1)
            
        reward += stagnation_punishment

        delta_current_reward = 0
        #! drobna nagroda za poprawę currentalue v 
        #! Uwaga nagroda ta jest zwiększona aż do 2.0 ale za to skaluje się z odległością między current a best czyli tym bardziej go nagdzadzamy im bardziej current zbliża się do obecnego best value 
        delta_current = self.run_history[-1][0] - new_observation[0]
        if delta_current > 0:
            delta_current_reward = min(4.0* delta_current ** 0.2,2) * (1 - max(min((new_observation[0] - new_observation[1])*3.0,0.9),0.1))
        reward += delta_current_reward

        reward = max(min(reward,self.norm_reward_scale),-self.norm_reward_scale)/self.norm_reward_scale
        if self.SA.steps_done < self.max_steps:
            is_terminated = False
        else:
            is_terminated = True
            self.done = True

        self.run_history.append( new_observation + [stagnation_punishment/self.norm_reward_scale,delta_current_reward/self.norm_reward_scale,self.current_temp,reward])
        self.total_reward += reward
        return new_observation, reward , is_terminated, False, self.info()



    C   C   C   C   C   C   C   C 
    zmiana parametrów uczenia 
    Parameter	Current	Suggested
    clip_coef	0.2	    0.1
    ent_coef	0.01	0.05
    update_epch	5	    3
    min_lr	    1e-8	1e-6
    vf_coef 	0.5	    0.25
    
    ustawiamy delty na 0 na razie (tak żeby nie dodawać więcej szumu do uczenia (to chyba był błąd można sprubować teraz przeuczyć tą sieć gla poprawnych wartości w deltach ale raczej ta sieć na nie wiele się już nada)), 
    
    
    większa sieć PPO_v2
    
    
    ustawiamy dłuższe ucznie 

    PARAMETRY uczenia
            #params
        # parametry związane GAE
        self.use_gae = True
        self.gamma = 0.97
        self.gae_lambda = 0.9
        # parametry związane z lr i jego updatem
        self.starting_lr = 0.00035 
        self.min_lr = 5e-6
        self.update_lr = True
        # podstawowe okreslające uczenie
        self.seed = 1
        self.num_envs = 3
        self.num_steps = 256 # ilość symulatnicznych kroków wykonanych na środowiskach podczas jednego batcha zbieranych danych o srodowiskach
        self.num_of_minibatches = 5 #(ustaw == num_envs) dla celów nie gubienia żadnych danych i żeby się liczby ładne zgadzały
        self.total_timesteps = 50000000 # określamy łączną maksymalna ilosć korków jakie łącznie mają zostać wykonane w środowiskach
        self.lr_cycle = int(self.total_timesteps / 2)
        # batch to seria danych w uczeniu, czyli na jedną pętlę zmierzemy tyle danych łącznie, a minibatch to seria ucząća i po seri zbierania danych, rozbijamy je na num_of_minibatches podgrup aby na tej podstawie nauczyć czegoś agenta
        self.batch_size = int(self.num_envs * self.num_steps)# training_batch << batch treningu określa ile łączeni stepów środowisk ma być wykonanych na raz przed updatem sieci na podstwie tych kroków
        self.minibatch_size = int(self.batch_size // self.num_of_minibatches)# rozmiar danych uczących na jeden raz
        print("total_timesteps:",self.total_timesteps)
        self.update_epochs = 3 # uwaga tutaj ustalamy, ile razy chcemy przejść przez cały proces uczenia na tych samych danych

        self.use_adv_normalization = True # flaga która decyduje czy adventage powinno być normalizowane

        #clipping params
        self.clip_coef = 0.125 # używane do strategi clippingu zaproponowanego w PPO
        self.clip_vloss = False # ! UWAGA WYŁĄCZLIŚMY BO DEEPSEEK MÓWI ŻE LEPSZE DO RZADKO SPOTYKANYCH NAGRÓD
        self.max_grad_norm = 0.5 # maksymalna zmiana wag w sieci 

        #Entropy loss params
        self.ent_coef = 0.05 # w jakim stopniu maksymalizujemy enthropy w porównaniu do minimalizacji błędu wyjścia sieci
        self.vf_coef = 0.25 # w jakim stopniu minimalizujemy value loss w porównaniu do minimalizowania błędu na wyjściu sieci

        # parametr ograniczający zbyt duże zmiany w kolejnych iteracjach
        self.target_kl = None # defaoult_value = 0.015





        D   D   D   D D   D   D   D


        Zmiana sterowania:
        self.current_temp += self.starting_temp * (self.actions[action_number] - 1 )#= 0.9 * self.current_temp * self.actions[action_number] + self.current_temp * 0.1
        if self.current_temp < self.min_temp:
            was_temp_lower_than_min = True
            self.current_temp = self.min_temp


        NAGRODA:
        Eliminujemy karę za zbyt niską temperaturę (ta kara zbyt ogranicza agenta!!!!) wychąłdzanie to nic złego jak pokazują inne strategie


        E   E   E   E   E   E   E   E   E   E   E

        UCZENIE BEZ INFO O HISTORII CZEGOKOLWIEK NIE MA SENSU 

        OBS:
        DODAJEMY HISTORIE TEMPERATURY W POSTACI: mean, std5,std25,trend5,trend25

        Test czy zwiększenie steps per temperature do 10 zwiększy skutecznosc sieci

                                                                UWAGA TEGO NIE TESTOWANO NA POPRAWNYCH DANYCH


        F F F F F

        zmieniono nagrody na bardziej adekwatne do nowego sterowania pierwsze ustawienie parametrów nowej nagrody

        self.temp_history_size = 50
        self.temp_short_size = 10
                                                                UWAGA TEGO NIE TESTOWANO NA POPRAWNYCH DANYCH (w run test nie dodwano temp_)

        G G G G 

        zbalansowano nagrody tak aby poprawka faktycznie była znacznie większa niżwszystkie pozostałe

        self.temp_history_size = 25
        self.temp_short_size = 5



        G2 G2 G2 

        dodatkowy balans nagród na podstawie tensorborda

        trendy: good_trends += 0.02 zamiast 0.1

        noImpr  - new_observation[3] * 0.025 (0.1)

        deltaC 0.0075 (0.02)


        zwiększono częstotliwość występowania kary za zbyt gwałtowne zmiany i o ciupinkę zmniejszono samą karę 

                too_fast_changes = 0
        if self.use_time_temp_info:
            if new_observation[-4]>0.01 or new_observation[-3]>0.0175: #[temp_mean, temp_std_fresh,temp_std_full, temp_trend_fresh,temp_trend_full]
                too_fast_changes -= 0.075
        reward += too_fast_changes




        #! nowe w G2
        #? drobna nagroda za utrzymywanie małych (bliskich 0) wartości trendu w okolicach wychładzania
        cold_seraching = 0
        if self.use_time_temp_info:
            if new_observation[-5]< 0.015 and abs(new_observation[-1]) < 0.005: #[temp_mean, temp_std_fresh,temp_std_full, temp_trend_fresh,temp_trend_full]
                cold_seraching += 0.005
        reward += cold_seraching
        self.total_cold_slow_changes += cold_seraching


        dodatkowo podniesiono mnożniki za kary w korkach w cieple /zimnie do 0.2 (0.175) i dodano 

        #! nowy element w G2. zerujemy karę za przebywanie w zimnie jak udało nam się faktycznie coś odnaleść
        if self.steps_without_correction <= 1:
            self.stesp_in_cold = 0
        
        if(self.stesp_in_cold > 5):#! nowy element w G2, opuźniamy karę za chłodzenie
            cold_walk_punishment -= (self.stesp_in_cold - 5)/self.SA_steps * 0.2